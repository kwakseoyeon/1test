{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VdKDqIJGxXo",
        "outputId": "1894c941-69c7-418b-987b-86f7d2e533f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.13)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.29 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.29)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.36)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.37)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.14.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.1.36)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.14.3)\n",
            "Requirement already satisfied: tiktoken<1,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.6.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain_openai) (0.1.37)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain_openai) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain_openai) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain_openai) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain_openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.33->langchain_openai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain_openai) (3.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.33->langchain_openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.33->langchain_openai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.33->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.33->langchain_openai) (2.0.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP4vp6xXpFrN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCyakdVyKbgp"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "chat_model = ChatOpenAI()\n",
        "output_parser = StrOutputParser() # Message의 content를 추출해서 string으로 변환\n",
        "\n",
        "# 체인구성 prompt_template > chat_model\n",
        "# 호출은 되지 않는다. 정의만 한다\n",
        "chain = prompt_template | chat_model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOmFdnFYu69R",
        "outputId": "2df1fe67-1e19-4c3c-c0b6-fbfb9d344a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='tell me a short joke about {topic}'))])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7cdedef3ba30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7cdedef38280>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
              "| StrOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# invoke 인자로 dict를 넘깁니다.\n",
        "# 그 이유가 prompt_template이 dict를 받기 때문입니다.\n",
        "\n",
        "chain.invoke({\"topic\": \"ice cream\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6grFdlXFcObi",
        "outputId": "0f78c63a-e1f0-417f-ff2b-5937c86d3f38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why did the ice cream truck break down? Because it had too many \"scoops\"!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "chat_model = ChatOpenAI()\n",
        "output_parser = StrOutputParser() # Message의 content를 추출해서 string으로 변환\n",
        "\n",
        "# 체인구성 prompt_template > chat_model\n",
        "# 호출은 되지 않는다. 정의만 한다\n",
        "chain = {\"topic\": \"ice cream\"} | prompt_template | chat_model | output_parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "id": "cTuv9J7rwaAa",
        "outputId": "f0209321-3029-486b-c0ce-155523f238bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-6f0f6c927411>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 체인구성 prompt_template > chat_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 호출은 되지 않는다. 정의만 한다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"topic\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"ice cream\"\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mprompt_template\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mchat_model\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0moutput_parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m__ror__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    432\u001b[0m     ) -> RunnableSerializable[Other, Output]:\n\u001b[1;32m    433\u001b[0m         \u001b[0;34m\"\"\"Compose this runnable with another object to create a RunnableSequence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mRunnableSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerce_to_runnable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     def pipe(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   4867\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRunnableLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4868\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4869\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnableParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4870\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4871\u001b[0m         raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _RunnableParallel__steps, **kwargs)\u001b[0m\n\u001b[1;32m   2952\u001b[0m         \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2953\u001b[0m         super().__init__(  # type: ignore[call-arg]\n\u001b[0;32m-> 2954\u001b[0;31m             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcoerce_to_runnable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2955\u001b[0m         )\n\u001b[1;32m   2956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2952\u001b[0m         \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2953\u001b[0m         super().__init__(  # type: ignore[call-arg]\n\u001b[0;32m-> 2954\u001b[0;31m             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcoerce_to_runnable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2955\u001b[0m         )\n\u001b[1;32m   2956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   4869\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnableParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4870\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4871\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m   4872\u001b[0m             \u001b[0;34mf\"Expected a Runnable, callable or dict.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4873\u001b[0m             \u001b[0;34mf\"Instead got an unsupported type: {type(thing)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "chat_model = ChatOpenAI()\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = {\"topic\": RunnablePassthrough()} | prompt_template | chat_model | output_parser"
      ],
      "metadata": {
        "id": "TSEzYgZowP-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tswryvm1vS3J",
        "outputId": "6225e6a9-fedb-433a-b128-ba6167a68aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\n",
              "  topic: RunnablePassthrough()\n",
              "}\n",
              "| ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='tell me a short joke about {topic}'))])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7cdedf0087f0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7cdedf0e84c0>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
              "| StrOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# invoke할 때, 굳이 topic 키로 dict로 넣어야 하나? 그냥 문자열로 넣으면 안될까?\n",
        "# 문자열을 받아서, chain에 전달됨\n",
        "# prompt_template의 입력 형식 dict로 받아야한다.\n",
        "# 인자를 value로 dict를 구성해서 넘거야겠다.\n",
        "# \"ice cream\" > RunnablePassthrough() > {\"topic\": RunnablePassthrough()} > prompt_template\n",
        "chain.invoke(\"ice cream\")\n",
        "\n",
        "# RunnablePassthrough() = \"ice cream\"\n",
        "\n",
        "# 질문 : RunnablePassthrough() 형식이 문자열인가요?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jtq6IM4W7hJ7",
        "outputId": "e83554be-d901-44cb-9798-13fed6b40890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why did the ice cream truck break down? It couldn't handle the rocky road!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 답 : invoke의 형식을 그대로 따릅니다. 만약 문자열이면 문자열, dict이면 dict입니다.\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "chat_model = ChatOpenAI(model=\"gpt-4\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# 이렇게도 가능합니다.\n",
        "chain = RunnablePassthrough() | prompt_template | chat_model | output_parser\n",
        "\n",
        "chain.invoke({\"topic\": \"ice cream\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3VM3a27B8QHh",
        "outputId": "c9621e88-7e5e-498d-ddfc-424216b82f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don't ice creams ever get into arguments? \\n\\nBecause they always stay cool!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAO_ZyjNV-Uq",
        "outputId": "f8afce98-773d-4d3e-c5ac-65a50e1c94e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnablePassthrough()\n",
              "| ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='tell me a short joke about {topic}'))])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7cdededfefe0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7cdedefaa680>, model_name='gpt-4', openai_api_key=SecretStr('**********'), openai_proxy='')\n",
              "| StrOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),"
      ],
      "metadata": {
        "id": "C4LZLJo6W0S2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdb52d7b-7775-4b2c-fa52-d41454b7e0a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(RunnableAssign(mapper={\n",
              "   mult: RunnableLambda(lambda x: x['num'] * 3)\n",
              " }),)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),"
      ],
      "metadata": {
        "id": "oFc86KjaWnt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82897e19-129a-4f35-ffed-50856efc68f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(RunnableAssign(mapper={\n",
              "   mult: RunnableLambda(lambda x: x['num'] * 3)\n",
              " }),)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# object approach\n",
        "chain = a.__or__(b)\n",
        "chain(\"some input\")\n",
        "\n",
        "# pipe approach\n",
        "chain = a | b\n",
        "chain(\"some input\")\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Y621fdW6dfY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Runnable:\n",
        "\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "\n",
        "    # 이 메서드는 파이썬의 비트 OR 연산자(|)를 오버로드합니다.\n",
        "    def __or__(self, other):\n",
        "\n",
        "        def chained_func(*args, **kwargs):\n",
        "            # the other func consumes the result of this func\n",
        "            return other(self.func(*args, **kwargs))\n",
        "\n",
        "        return Runnable(chained_func)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.func(*args, **kwargs)"
      ],
      "metadata": {
        "id": "eQ7_Yb65bIpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_five(x):\n",
        "    return x + 5\n",
        "\n",
        "def multiply_by_two(x):\n",
        "    return x * 2"
      ],
      "metadata": {
        "id": "OBMlj0tkbM-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrap the functions with Runnable\n",
        "add_five = Runnable(add_five)\n",
        "multiply_by_two = Runnable(multiply_by_two)"
      ],
      "metadata": {
        "id": "KdwcLqVJbOS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chain the runnable functions together\n",
        "chain = add_five | multiply_by_two"
      ],
      "metadata": {
        "id": "6kfPseF3_dBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# invoke the chain\n",
        "chain(3)  # we should return 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeFvGwGZ_eVH",
        "outputId": "37e17cbb-2375-4853-9fa9-f8739d785287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run them using the object approach\n",
        "chain = add_five.__or__(multiply_by_two)\n",
        "chain(3)  # should return 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg3ciXXqHCCi",
        "outputId": "38afe078-f591-4395-94f2-9081af190973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num2ko(x):\n",
        "    return str(x) + \"입니다.\""
      ],
      "metadata": {
        "id": "WGXEWY7bItV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = add_five | multiply_by_two | num2ko"
      ],
      "metadata": {
        "id": "IOCma1nZI3g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain(3)\n",
        "\n",
        "# num2ko는 Runnable이 아닌데, 호출이 되나요?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VDJt5MYkI4p0",
        "outputId": "26b2b0be-d9a4-4b49-b1ef-718a1ad3d07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'16입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# 템플릿 만드는 방법\n",
        "\n",
        "# 기획\n",
        "# \"Please generate dialogue sentences in English on the topic of health for a beginner level.\"\n",
        "# \"Please generate dialogue sentences in Korean on the topic of AI for an intermediate level.\"\n",
        "\n",
        "# 양식과 변수를 분리하기\n",
        "# \"Please generate dialogue sentences in (English) on the topic of (health) for a (beginner) level.\"\n",
        "# \"Please generate dialogue sentences in (Korean) on the topic of (AI) for an (intermediate) level.\"\n",
        "\n",
        "# \"Please generate dialogue sentences in (language) on the topic of (topic) for a/an (level) level.\"\n",
        "\n",
        "# 양식 조정하기\n",
        "# \"Please generate dialogue sentences in {language} on the topic of {topic} for a level of {level}.\""
      ],
      "metadata": {
        "id": "mbKP6cxOiQLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"Please generate dialogue three sentences in {language} on the topic of {topic} for a level of {level}.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt_template | chat_model # | StrOutputParser()\n",
        "\n",
        "print(chain)\n",
        "\n",
        "output = chain.invoke({\"language\" : \"English\", \"topic\" : \"travel\", \"level\" : \"beginner\"})\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "0CJg_YS9iSSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942bc996-e8d6-48fc-e3bb-e79867828af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first=ChatPromptTemplate(input_variables=['language', 'level', 'topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'level', 'topic'], template='Please generate dialogue three sentences in {language} on the topic of {topic} for a level of {level}.'))]) last=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7cdededfefe0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7cdedefaa680>, model_name='gpt-4', openai_api_key=SecretStr('**********'), openai_proxy='')\n",
            "content=\"A: Hello, have you ever traveled abroad?\\nB: No, I haven't but I would love to visit Italy one day.\\nA: That sounds wonderful, Italy is known for its beautiful scenery and delicious food.\" response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 25, 'total_tokens': 69}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 변수 중에 시스템에서 입력해야할 것과 사용자가 입력해야할 것 분리\n",
        "# language : 설정\n",
        "# topic : 바뀐다.\n",
        "# level : 설정\n",
        "\n",
        "def get_learning_language(_):\n",
        "    print(\"###\")\n",
        "    print(_)\n",
        "    print(\"in get_learning_language\")\n",
        "    print(\"###\")\n",
        "    return \"English\"\n",
        "\n",
        "def get_learning_level(_):\n",
        "    print(\"###\")\n",
        "    print(_)\n",
        "    print(\"in get_learning_level\")\n",
        "    print(\"###\")\n",
        "    return \"beginner\"\n",
        "\n",
        "# 그럼 이 함수를 체인 내에서 어떻게 호출해야하는가?"
      ],
      "metadata": {
        "id": "WavqPIUFiUrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 방법 1\n",
        "\n",
        "template = \"Please generate dialogue three sentences in {language} on the topic of {topic} for a level of {level}.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt_template | chat_model| StrOutputParser()\n",
        "\n",
        "print(chain)\n",
        "\n",
        "output = chain.invoke({\"language\" : get_learning_language(''), \"topic\" : \"travel\", \"level\" : get_learning_level('')})\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "-FI_nBMrUOR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47ab239-34b9-4dfc-ca81-f6f02a913f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first=ChatPromptTemplate(input_variables=['language', 'level', 'topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'level', 'topic'], template='Please generate dialogue three sentences in {language} on the topic of {topic} for a level of {level}.'))]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7cdededfefe0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7cdedefaa680>, model_name='gpt-4', openai_api_key=SecretStr('**********'), openai_proxy='')] last=StrOutputParser()\n",
            "###\n",
            "\n",
            "in get_learning_language\n",
            "###\n",
            "###\n",
            "\n",
            "in get_learning_level\n",
            "###\n",
            "A: Have you ever traveled outside of your country?\n",
            "B: No, I haven't. I have always wanted to explore new places.\n",
            "A: Traveling is a great way to learn about different cultures and lifestyles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 방법2\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"Please generate dialogue three sentences in {language} on the topic of {topic} for a level of {level}.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(language = get_learning_language,\n",
        "                               level = get_learning_level)\n",
        "    | prompt_template\n",
        "    | chat_model\n",
        ")\n",
        "\n",
        "# {\"topic\" : \"travel\"} > dict language, level 추가 > {\"topic\", ... \"language\", ... \"level\" ...} dict가 완성\n",
        "# > prompt_template > chat_model\n",
        "\n",
        "print(chain)\n",
        "\n",
        "output = chain.invoke({\"topic\" : \"travel\"})\n",
        "\n",
        "print(output)\n",
        "\n",
        "# 넘어오는 인자에 추가적으로 key와 value를 넣고 싶을 때, RunnablePassthrough을 사용한다."
      ],
      "metadata": {
        "id": "7zoOVeEuiW5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49da11e8-d103-49ca-ba21-81f700f9f023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first=RunnableAssign(mapper={\n",
            "  language: RunnableLambda(get_learning_language),\n",
            "  level: RunnableLambda(get_learning_level)\n",
            "}) middle=[ChatPromptTemplate(input_variables=['language', 'level', 'topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'level', 'topic'], template='Please generate dialogue three sentences in {language} on the topic of {topic} for a level of {level}.'))])] last=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7cdedefc20b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7cdedef39690>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
            "###\n",
            "{'topic': 'travel'}\n",
            "in get_learning_language\n",
            "###\n",
            "###\n",
            "{'topic': 'travel'}\n",
            "in get_learning_level\n",
            "###\n",
            "content='1. \"I want to go on a trip to a new country.\"\\n2. \"Where should I go for my next vacation?\"\\n3. \"I love exploring new places and trying new foods when I travel.\"' response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 25, 'total_tokens': 68}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "id": "0UCwGveJyklR",
        "outputId": "d5be17c9-028e-4245-c095-20c92d9159d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableAssign(mapper={\n",
              "  language: RunnableLambda(get_learning_language),\n",
              "  level: RunnableLambda(get_learning_level)\n",
              "})\n",
              "| ChatPromptTemplate(input_variables=['language', 'level', 'topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'level', 'topic'], template='Please generate dialogue three sentences in {language} on the topic of {topic} for a level of {level}.'))])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7cdedefc20b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7cdedef39690>, openai_api_key=SecretStr('**********'), openai_proxy='')"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"Please generate dialogue three sentences in {language} on the topic of {topic} for a level of {level}.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\"topic\" : RunnablePassthrough()}\n",
        "    | RunnablePassthrough.assign(language = get_learning_language,\n",
        "                                 level = get_learning_level)\n",
        "\n",
        "    | prompt_template\n",
        "    | chat_model\n",
        ")\n",
        "\n",
        "print(chain)\n",
        "\n",
        "output = chain.invoke(\"travel\")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "T90SuR4IiY5C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc19252-b0ff-45ef-eb80-fe1dce3dc5f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first={\n",
            "  topic: RunnablePassthrough()\n",
            "} middle=[RunnableAssign(mapper={\n",
            "  language: RunnableLambda(get_learning_language),\n",
            "  level: RunnableLambda(get_learning_level)\n",
            "}), ChatPromptTemplate(input_variables=['language', 'level', 'topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'level', 'topic'], template='Please generate dialogue three sentences in {language} on the topic of {topic} for a level of {level}.'))])] last=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7cdededfefe0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7cdedefaa680>, model_name='gpt-4', openai_api_key=SecretStr('**********'), openai_proxy='')\n",
            "###\n",
            "{'topic': 'travel'}\n",
            "in get_learning_language\n",
            "###\n",
            "###\n",
            "{'topic': 'travel'}\n",
            "in get_learning_level\n",
            "###\n",
            "content='A: Do you like to travel?\\nB: Yes, I love it. Where is your favorite place to travel?\\nA: My favorite place is Paris, I love the culture and food there.' response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 25, 'total_tokens': 65}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "User\n",
        "\"언어 레벨에 따라 주어진 주제에 대해서 특정 언어 대화 문장 을 만들어줘.\"를 문장을 프롬프트에 맞게 다듬어줘.\n",
        "\n",
        "2 / 2\n",
        "\n",
        "ChatGPT\n",
        "\"언어 레벨에 따라 주어진 주제에 대한 특정 언어의 대화 문장을 생성해주세요.\"\n",
        "\n",
        "User\n",
        "\"언어 레벨에 따라 주어진 주제에 대한 특정 언어의 대화 문장을 생성해주세요.\"\n",
        "\n",
        "위 문장에 입력 인자를 \"레벨\", \"주제\", \"언어\"로 표시해\n",
        "\n",
        "2 / 2\n",
        "\n",
        "ChatGPT\n",
        "\"레벨에 따라 주제에 대한 언어의 대화 문장을 생성해주세요.\"\n",
        "\n",
        "레벨: 사용자의 언어 능력 수준\n",
        "주제: 대화의 주제\n",
        "언어: 대화를 진행할 언어\n",
        "User\n",
        "\"레벨에 따라 주제에 대한 언어의 대화 문장을 생성해주세요.\" 을 영어로\n",
        "ChatGPT\n",
        "\"Please generate dialogue sentences in the specified language for the given topic, according to the language level.\"\n",
        "\n",
        "User\n",
        "\"Please generate dialogue sentences in the specified language for the given topic, according to the language level.\"\n",
        "\n",
        "위 문장을 아래 정보를 이용해서, 다시 수정해\n",
        "- 쉬운 레벨, 건강, 영어\n",
        "- 중간 레벨, AI, 한국어\n",
        "\n",
        "\n",
        "2 / 2\n",
        "\n",
        "ChatGPT\n",
        "\"Please generate dialogue sentences in English on the topic of health for a beginner level, and in Korean on the topic of AI for an intermediate level.\"\n",
        "\n",
        "User\n",
        "두 문장을 나누어\n",
        "ChatGPT\n",
        "\"Please generate dialogue sentences in English on the topic of health for a beginner level.\"\n",
        "\"Please generate dialogue sentences in Korean on the topic of AI for an intermediate level.\"\n",
        "User\n",
        "\"Please generate dialogue sentences in English on the topic of health for a beginner level.\"\n",
        "\"Please generate dialogue sentences in Korean on the topic of AI for an intermediate level.\"\n",
        "\"Please generate dialogue sentences in {language} on the topic of {topic} for an {level} level.\"\n",
        "\n",
        "위에서 a {level} an {level}로 된다. 해결법은?\n",
        "ChatGPT\n",
        "For the phrase to grammatically adjust according to the article \"a\" or \"an\" before the word \"level,\" based on whether the level starts with a vowel sound or not, you can use a conditional structure in your sentence. However, since we're working with a static text and not a dynamic language processing tool here, I'll provide a guideline on how to adjust it manually:\n",
        "\n",
        "Use \"an\" before levels that start with a vowel sound (e.g., \"intermediate\" doesn't start with a vowel sound but \"beginner\" does not either; however, this rule is about sounds not just letters, so it's tricky without specific examples of vowel-starting levels like \"advanced\" where the \"a\" in \"advanced\" doesn't actually make it \"an advanced\" due to the consonant sound it starts with).\n",
        "Use \"a\" before levels that start with a consonant sound.\n",
        "Given this, the correct approach for each specific level needs to be determined by the sound of the first letter of the level descriptor (e.g., \"an intermediate level\" is incorrect because \"intermediate\" starts with a consonant sound, so it should be \"a intermediate level\", but normally \"intermediate\" would just use \"a\").\n",
        "\n",
        "To universally apply this in a sentence template, you might rephrase to avoid the issue:\n",
        "\n",
        "\"Please generate dialogue sentences in {language} on the topic of {topic} for a level of {level}.\"\n",
        "\n",
        "By structuring the sentence this way, you avoid the need to choose between \"a\" or \"an\" based on the level descriptor that follows.\n",
        "\n",
        "User\n",
        "# \"Please generate dialogue sentences in {language} on the topic of {topic} for a level of {level}.\"\n",
        "\n",
        "여기서 넣을 수 있는 변수 샘플 여러개\n",
        "ChatGPT\n",
        "Here are several examples of variables you can insert into the sentence \"Please generate dialogue sentences in {language} on the topic of {topic} for a level of {level}.\":\n",
        "\n",
        "language: English, topic: travel, level: beginner\n",
        "\n",
        "\"Please generate dialogue sentences in English on the topic of travel for a level of beginner.\"\n",
        "language: French, topic: cooking, level: intermediate\n",
        "\n",
        "\"Please generate dialogue sentences in French on the topic of cooking for a level of intermediate.\"\n",
        "language: Spanish, topic: technology, level: advanced\n",
        "\n",
        "\"Please generate dialogue sentences in Spanish on the topic of technology for a level of advanced.\"\n",
        "language: Korean, topic: culture, level: beginner\n",
        "\n",
        "\"Please generate dialogue sentences in Korean on the topic of culture for a level of beginner.\"\n",
        "language: Japanese, topic: sports, level: intermediate\n",
        "\n",
        "\"Please generate dialogue sentences in Japanese on the topic of sports for a level of intermediate.\"\n",
        "language: German, topic: environmental issues, level: advanced\n",
        "\n",
        "\"Please generate dialogue sentences in German on the topic of environmental issues for a level of advanced.\"\n",
        "These examples cover a range of languages, topics, and proficiency levels to illustrate how you can customize the sentence template for various dialogue sentence generation requests.\n",
        "\n",
        "User\n",
        "외국어 연습을 하기 위해서는 3 sentences가 좋은가 4 sentences가 좋은가?\n",
        "ChatGPT\n",
        "외국어 연습을 위한 문장의 수는 학습자의 목표, 집중력, 그리고 시간에 따라 달라질 수 있습니다. 일반적으로, 연습을 위한 문장의 수는 다음과 같은 요소들을 고려하여 결정되어야 합니다:\n",
        "\n",
        "학습 목표: 학습자가 단기간 내에 특정 주제나 어휘에 초점을 맞추려는 경우, 더 많은 문장을 연습하는 것이 도움이 될 수 있습니다. 반면, 특정 구조나 문법 규칙을 연습하는 데에는 적은 수의 문장으로도\n",
        "User\n",
        "dialogue 인 경우\n",
        "ChatGPT\n",
        "외국어 연습을 위한 대화(dialogue) 연습의 경우, 문장의 수는 연습의 목표와 상황에 따라 달라질 수 있습니다. 그러나 대화 스킬을 향상시키기 위해서는 다음과 같은 이유로 4개 문장이 3개 문장보다 더 유리할 수 있습니다:\n",
        "\n",
        "대화의 흐름 이해: 4개의 문장은 3개의 문장보다 더 많은 정보와 대화의 전환을 제공하여 학습자가 대화의 흐름을 이해하는 데 도움을 줄 수 있습니다.\n",
        "복합 대화 연습: 더 많은 문장을 사용하면 질문, 응답, 추가 설명, 반응 등 다양한 대화 요소를 연습할 수 있습니다. 이는 실제 대화에서 자연스러운 의사소통 능력을 키우는 데 유리합니다.\n",
        "언어 다양성: 4개의 문장은 더 많은 어휘와 문법 구조를 사용하게 하여 언어 학습의 다양성과 깊이를 증가시킵니다.\n",
        "상호작용 기회 증가: 대화에서 상호작용은 중요한 요소입니다. 4개의 문장을 사용함으로써 상대방과의 상호작용을 더 많이 연습할 수 있으며, 이는 실제 상황에서의 의사소통 능력 향상에 도움이 됩니다.\n",
        "그러나 학습자의 수준과 특정 학습 목표에 따라 다를 수 있습니다. 초보자의 경우, 3개의 문장으로 시작하여 기본적인 대화를 연습하고, 점차 문장 수를 늘려가며 복잡한 대화 구조에 익숙해지는 것이 좋을 수 있습니다. 중급 이상의 학습자라면, 4개 이상의 문장을 포함한 더 복잡한 대화 연습이 더 적합할 수 있습니다.\n",
        "\n",
        "User\n",
        "template = \"Please generate dialogue three sentences in {language} on the topic of {topic} for a level of {level}.\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | chat_model\n",
        "\n",
        "print(chain)\n",
        "\n",
        "\"language\" : \"English\",\n",
        "\"level\" : \"beginner\"\n",
        "\n",
        "output = chain.invoke({\"topic\" : \"travel\"})\n",
        "\n",
        "print(output)\n",
        "\n",
        "여기서 language와 level는 시스템에서 수강 시에 미리 지정한 것을 사용하고자 한다. 즉 사용자가는 주제만 바꾸는 식이다. 그럼 시스템에서 지정된 것을 호출 하는 함수명으로 적합한 것은? language와 level를 따로 부른다.\n",
        "ChatGPT\n",
        "시스템에서 미리 지정된 language와 level 값을 호출하는 데 사용할 수 있는 함수명을 정의하려면, 명확하고 간결한 명명 규칙을 따르는 것이 좋습니다. 각각의 값을 따로 호출하는 경우, 함수명은 해당 값의 종류와 \"get\" 또는 \"fetch\"와 같은 동사를 결합하여 구성할 수 있습니다. 이러한 접근 방식은 함수의 목적을 명확히 전달하며, 어떤 종류의 값을 반환하는지 이해하기 쉽게 만듭니다.\n",
        "\n",
        "예를 들어, language와 level 값을 호출하는 데 적합한 함수명은 다음과 같습니다:\n",
        "\n",
        "getLanguage(): 현재 설정된 언어 값을 반환합니다.\n",
        "getLevel(): 현재 설정된 레벨 값을 반환합니다.\n",
        "이 함수들은 시스템에서 미리 지정된 값을 사용자나 다른 시스템 부분에 제공하는 역할을 합니다. 사용자는 주제만 선택하면, 이 함수들을 통해 현재 설정된 언어와 레벨에 따라 적절한 대화 문장을 생성할 수 있게 됩니다.\n",
        "\n",
        "따라서, 시스템에서 지정된 language와 level를 호출하는 함수명으로 getLanguage()와 getLevel()이 적합합니다. 이 함수들은 각각 현재 설정된 언어와 레벨 값을 반환하여, 대화 문장 생성 과정에서 사용될 수 있습니다.\n",
        "\n",
        "User\n",
        "get_my_language 에서 내가 배우고자 하는 랭퀴지로 바꾸려면?\n",
        "ChatGPT\n",
        "내가 배우고자 하는 언어를 반환하는 함수의 이름을 지정할 때, 함수명은 그 기능을 명확하게 반영해야 합니다. get_my_language에서 \"내가 배우고자 하는 언어\"로 명확히 하려면, 함수명을 더 직관적으로 바꿀 수 있습니다. 예를 들어, 배우고자 하는 언어를 반환하는 함수의 경우, 다음과 같은 이름을 고려할 수 있습니다:\n",
        "\n",
        "getTargetLanguage()\n",
        "getLearningLanguage()\n",
        "이러한 이름은 함수가 \"배우고자 하는 언어\"를 반환한다는 것을 더 명확하게 전달하며, 시스템 내에서 언어 학습 관련 설정이나 기능을 찾는 데 있어 직관적인 가이드를 제공합니다.\n",
        "\n",
        "\n",
        "\n",
        "User\n",
        "랭체인에서 템플릿안에 변수를 뭐라고 부르는가?\n",
        "ChatGPT\n",
        "랭체인(LangChain)이나 유사한 프로그래밍 환경에서 템플릿 안에 사용되는 변수를 일반적으로 \"placeholder\" 또는 \"template variable\"이라고 부릅니다. 이러한 변수는 템플릿 문자열에서 동적으로 교체될 값을 표시하기 위해 사용되며, 실행 시간에 실제 값으로 대체되어 최종 문자열을 생성합니다. Placeholder나 template variable은 템플릿을 더 유연하게 사용할 수 있게 해주며, 코드의 재사용성을 높여줍니다."
      ],
      "metadata": {
        "id": "HNh2Zj9sXY-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "def get_topic_language(input_situation):\n",
        "\n",
        "    system_content_template=\"Please generate topic regarding a situation like {situation}\"\n",
        "    system_message_prompt_template = SystemMessagePromptTemplate.from_template(system_content_template)\n",
        "    chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt_template])\n",
        "    chat_model = ChatOpenAI(temperature=0)\n",
        "    messages = chat_prompt_template.format_prompt(situation = input_situation['situation']).to_messages()\n",
        "    response =  chat_model(messages)\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "0-CS-UtwZUDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토픽을 주는 것이 아니라, 어떤 상황을 설명해주면 거기에 맞는 토픽을 자동으로 설정해준다.\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"Please generate dialogue three sentences in {language} on the topic of {topic} for a level of {level}.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(topic = get_topic_language,\n",
        "                                 language = get_learning_language,\n",
        "                                 level = get_learning_level)\n",
        "\n",
        "    | prompt_template\n",
        "    | chat_model\n",
        ")\n",
        "\n",
        "output = chain.invoke({\"situation\" : \"온라인 강의하는 상황\"})\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "y2iIMCBgYyUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816af021-9359-4dd4-b1dd-fd32c58e39ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###\n",
            "{'situation': '온라인 강의하는 상황'}\n",
            "in get_learning_language\n",
            "###\n",
            "###\n",
            "{'situation': '온라인 강의하는 상황'}\n",
            "in get_learning_level\n",
            "###\n",
            "content='1. \"Online learning can be both challenging and rewarding. It requires self-discipline but offers flexibility. Teachers may struggle to engage students but can reach a wider audience.\"\\n\\n2. \"Staying motivated in online classes can be tricky. Setting personal goals can help, as can actively participating in discussions. Remember, it\\'s about your growth, not just ticking off tasks!\"\\n\\n3. \"Technology has transformed education. It\\'s not just about Zoom classes, it\\'s about interactive learning platforms and a wealth of resources at our fingertips. But remember, tech is a tool, not a solution.\"\\n\\n4. \"Creating a supportive online environment is crucial. We all need to feel comfortable and motivated to learn. This means respecting one another and making sure everyone feels included.\"\\n\\n5. \"Managing time effectively is key in online learning. Prioritize tasks, create a schedule, and don\\'t forget to take breaks. Remember, it\\'s a marathon, not a sprint.\"\\n\\n6. \"Online learning can promote inclusivity. It allows those who may not be able to attend traditional classes to learn. It\\'s a chance for education to reach everyone, no matter their circumstances.\"\\n\\n7. \"Online education has a bright future. It\\'s a space for innovation, for new teaching methods and tools. It\\'s a chance to rethink how we educate and learn.\"\\n\\n8. \"Prolonged online learning can have psychological effects. Students and teachers may feel isolated or overwhelmed. It\\'s important to check in with each other and ensure mental health is a priority.\"\\n\\n9. \"Online learning has ethical considerations. Academic integrity must be maintained, and privacy respected. It\\'s about creating a safe and respectful environment for everyone.\"\\n\\n10. \"Successful online collaboration involves clear communication. Respect different time zones, respond promptly, and remember, everyone\\'s voice matters. Together, we can make virtual classrooms effective and enjoyable!\"' response_metadata={'token_usage': {'completion_tokens': 374, 'prompt_tokens': 240, 'total_tokens': 614}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM기반 서비스 개발을 위한 프롬프트 만들기"
      ],
      "metadata": {
        "id": "FL3VcOXYinpy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P4FPvFprizmz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}